{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Quantum Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies\n",
    "!pip install tensorflow\n",
    "!pip install -q tf-agents\n",
    "!pip install qiskit \n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall protobuf --yes\n",
    "!pip3 uninstall python-protobuf --yes\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Structure\n",
    "\n",
    "The goal of the project is to find an optimal pulse shape for a quantum gate. In this hackathon project, the aim is to do this without a physical model of the noise, but by learning the optimal pulse with reinforcement learning.\n",
    "\n",
    "In general in Reinforcement Learning (RL), an agent tries to learn an optimal behaviour in an environment it can (partially) observe the results of its actions on. The agent acts on the environment with some action, which then transitions into a new state and the agent receives 'feedback' in terms of a reward. It is now the goal to optimise the reward.\n",
    "![rl_illustration](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/497px-Reinforcement_learning_diagram.svg.png)\n",
    "\n",
    "### Agent: `agent.py`\n",
    "The agent in this project 'is the control hardware of a qubit'. As actions, it sends EM-pulses onto an imperfect qubit to achieve a certain quantum gate; it can observe the final state of the qubit and the reward is determined in terms of the fidelity of the final state and the theoretically optimal final state.\n",
    "\n",
    "This is implemented via [tensorflow agents](https://www.tensorflow.org/agents); a number of technical aspects resulting from that are implemented in `agent.py` (e.g. replay buffer, episodes, action distribution, ..).\n",
    "\n",
    "The pulse as an action is implemented as a piecewise constant function, i.e. the actual values that are learned is a list of amplitudes for a number of minimal time steps. E.g. this is how the agent samples an action from its action distribution, applies it to the environment (ie. the qubit) and receives the reward (fidelty):\n",
    "```python\n",
    "    def evaluate(self, tf_agent, eval_py_env):\n",
    "        eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "        num_episodes = 1\n",
    "        fidelity = []\n",
    "        actions = []\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            while not time_step.is_last():\n",
    "                action_step = tf_agent.policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                actions.append(action_step.action)\n",
    "        fidelity, state, pulse_prog = eval_py_env.get_state(actions)\n",
    "        return state, fidelity, actions, pulse_prog\n",
    "```\n",
    "\n",
    "### Environment: `environment.py`\n",
    "\n",
    "The environment consists of the qubit that is controlled by the pulses, here it is crucial to simulate a qubit on the pulse level - for this **qiskit `PulseSimulator`** is used.\n",
    "\n",
    "E.g. a pulse is created from the action..\n",
    "```python\n",
    "        with pulse.build(name=\"pulse_programming_in\", backend=armonk_backend\n",
    "        ) as pulse_prog:\n",
    "\n",
    "            dc = pulse.DriveChannel(0)\n",
    "            ac = pulse.acquire_channel(0)\n",
    "\n",
    "            for action in actions:\n",
    "                pulse.play([action] * self.interval_width, dc)\n",
    "            pulse.delay(self.interval_width * len(self.actions_list) + 10, ac)\n",
    "            mem_slot = pulse.measure(0)\n",
    "```\n",
    ".. to be used in the simulator:\n",
    "```python\n",
    "        backend_sim = PulseSimulator(system_model=armonk_model)\n",
    "        qobj = assemble(pulse_prog, backend=backend_sim, meas_return=\"avg\", shots=512)\n",
    "        sim_result = backend_sim.run(qobj).result()\n",
    "        vector = sim_result.get_statevector()\n",
    "        fid = state_fidelity(np.array([0, 1]), vector)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "in the training, an `Agent` (see `agent.py`) is created and the tensorflow agents framework is used to train the action distribution via reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pulseRL.environment import QiskitEnv\n",
    "import numpy as np\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from pulseRL.agent import Agent\n",
    "from qiskit.visualization import plot_bloch_multivector\n",
    "import matplotlib as plt\n",
    "\n",
    "# Learning Parameters\n",
    "num_iterations = 1000\n",
    "\n",
    "#In an iteration multiple episodes are collected together and a trajectory is built out of it.\n",
    "#Later these trajectory is used for learning. Trajectory is added to a replay buffer and analysed together.\n",
    "collect_episodes_per_iteration = 250  #\n",
    "replay_buffer_capacity = 2000  \n",
    "\n",
    "\n",
    "learning_rate = 1e-3  \n",
    "num_eval_episodes = 2  \n",
    "eval_interval = 50\n",
    "num_intervals = 10\n",
    "interval_length = 60\n",
    "\n",
    "\n",
    "\"\"\"Enviroment which make use of Qiskit Pulse Simulator and pulse builder to simulate \n",
    "the dynamics of a qubit under the influence of a pulse. The RL agent interact with this \n",
    "environment through action defined as pulse lenght. Here a constant pulse of amplitude 1 \n",
    "is used and applied for a time \"pulse width\". \"pulse width\" is the action that the agent \n",
    "takes here. The agent observes the state obtained with the action along with the Fidelity \n",
    "to the expected final state. Here initial state is fixed to |0> and target state is |1>\n",
    "\n",
    "The pulse is designed as follows\n",
    "  The process time is divided into \"num_intervals\" of length \"interval_length\".\n",
    "  For each interval a constant amplitude of range(0,1) is defined by the agent\n",
    "  delay the mesearement channel for num_intervals*interval_length + 10 time and make mesurement.\n",
    "TODO: Make the environement more gernect to handle different operators and initial states\"\"\"\n",
    "environment =  QiskitEnv(np.array([1,0]), num_intervals, interval_length)\n",
    "\n",
    "#convert the python environment to tensorflow compactible format for training.\n",
    "tf_dumm = tf_py_environment.TFPyEnvironment(environment)\n",
    "\"\"\"Get the reinfoce agent. Reward is the fielily to target state. Observation is the state\"\"\"\n",
    "agent = Agent(num_iterations, collect_episodes_per_iteration, replay_buffer_capacity, learning_rate, num_eval_episodes, eval_interval, num_intervals, interval_length)\n",
    "agent_reinforce = agent.get_agent(environment, 'reinforce', \"without_noise_trained\")\n",
    "train_results = agent.train(tf_dumm, agent_reinforce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(for the algorithm of train() see _`agent.py` _, this mainly uses tf_agent)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to save the policy for later usage:\n",
    "from tf_agents.policies import policy_saver\n",
    "policy_dir = \"policy\"\n",
    "tf_policy_saver = policy_saver.PolicySaver(agent_reinforce.policy)\n",
    "tf_policy_saver.save(policy_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = QiskitEnv(np.array([0,1]),5,100)\n",
    "vector, fid, action, pulse_prog = agent.evaluate(agent_reinforce, env_test)\n",
    "print(\"Fidelity : \", fid)\n",
    "control_pulse = [act.numpy()[0][0] for act in action]\n",
    "print(\"Control pulse\", control_pulse)\n",
    "env_test.get_state(control_pulse)\n",
    "#plot_bloch_multivector(vector)\n",
    "pulse_prog.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow.saved_model import load\n",
    "from tf_agents import policies\n",
    "from pulseRL.environment import QiskitEnv\n",
    "import warnings\n",
    "import logging, sys\n",
    "def evaluate(policy, eval_py_env):\n",
    "        eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "        num_episodes = 1\n",
    "        fidelity = []\n",
    "        actions = []\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                actions.append(action_step.action)\n",
    "        fidelity,state, pulse_prog = eval_py_env.get_state(actions)\n",
    "        return state, fidelity, actions,pulse_prog\n",
    "\n",
    "env_test = QiskitEnv(np.array([0,1]),5,100)\n",
    "policy_dir = \"best_policy\"\n",
    "saved_policy = load(policy_dir)\n",
    "state, fidelity, actions, pulse_prog = evaluate(saved_policy,env_test)    \n",
    "print(\"Showing the results of the best policy\")\n",
    "print(\"Fidelity : \",fidelity)\n",
    "print(\"Initial State: \", [1,0])\n",
    "print(\"Final State: \", state)\n",
    "print(\"\\n\\n\")\n",
    "pulse_prog.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.plot(train_results[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
